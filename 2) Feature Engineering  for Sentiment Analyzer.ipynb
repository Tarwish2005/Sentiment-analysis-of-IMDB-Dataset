{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86584429",
   "metadata": {},
   "source": [
    "## Importing all dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528d0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6d02b",
   "metadata": {},
   "source": [
    "## Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21565e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Youtube\\\\4th Video\\\\dataset\\\\IMDB-Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\Youtube\\\\4th Video\\\\dataset\\\\IMDB-Dataset.csv'"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "df = pd.read_csv(r'E:\\Youtube\\4th Video\\dataset\\IMDB-Dataset.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7142c7",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b8d7538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'until', 'wasn', 'because', 'have', 'are', \"you'll\", 'been', 'her', 'ourselves', 'why', 'his', 'it', 'that', \"weren't\", 'when', 'off', 'against', \"i'd\", 're', 'himself', \"we've\", 'whom', 'am', \"he'd\", 'mustn', 'themselves', 'this', 'a', \"we'd\", 'then', 'further', 'where', 'doing', \"wouldn't\", 'down', 'yours', 'would', \"we're\", \"it'll\", 'yourself', 'nor', 'these', 'might', 've', 'how', 'in', 'she', 'y', 'your', \"wasn't\", 'while', 'ma', \"couldn't\", \"won't\", 'below', 'there', 'had', 'should', 'he', 'o', 'an', 'up', 'haven', 'very', 'they', 'above', 'what', 'for', 'again', \"she'd\", 'myself', 'out', 'with', \"she's\", \"it'd\", \"shan't\", \"i'll\", 'you', 'll', 'is', 'their', \"needn't\", 'more', 'me', 'ours', \"hasn't\", 'those', 'most', 'such', 'i', 'each', \"i'm\", \"it's\", 'do', 't', 'our', 'won', 'at', 'no', 'before', 'who', 'does', \"didn't\", 'd', 'needn', 'aren', 'under', 'about', \"should've\", \"they've\", 'herself', 'if', 'by', \"haven't\", 'same', \"she'll\", 'my', 'theirs', 'hasn', 'couldn', 'mightn', 'on', 'but', 'didn', 'into', 'has', 'shouldn', \"you'd\", 'can', 'wouldn', \"he's\", 'once', \"we'll\", 'its', 'don', 'as', 'few', 'to', 'other', 'being', \"you've\", 'doesn', 'be', 'just', 'over', 'ain', 'all', 'after', 'both', 'hadn', 'weren', \"mustn't\", 'shan', 'm', 'which', 'only', 'any', 'shall', 'the', 'having', 'own', \"aren't\", \"they'd\", 'through', 'them', 'was', \"that'll\", 'him', 'and', 's', \"they'll\", \"you're\", 'so', 'some', 'between', 'than', 'now', \"shouldn't\", \"i've\", 'were', \"don't\", 'here', \"doesn't\", 'or', \"they're\", 'of', 'hers', 'will', \"hadn't\", 'we', 'itself', \"he'll\", 'could', \"isn't\", 'during', 'from', 'yourselves', 'did', 'too', \"mightn't\"}\n"
     ]
    }
   ],
   "source": [
    "#Customize stopword as per data\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"would\",\"shall\",\"could\",\"might\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")\n",
    "stop_words=set(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d08805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------Data Cleaning and Preprocessing pipeline----------------------------------'''\n",
    "\n",
    "#Removing special character\n",
    "def remove_special_character(content):\n",
    "    return re.sub('\\W+',' ', content )#re.sub('\\[[^&@#!]]*\\]', '', content)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_url(content):\n",
    "    return re.sub(r'http\\S+', '', content)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "# Expansion of english contractions\n",
    "def contraction_expansion(content):\n",
    "    content = re.sub(r\"won\\'t\", \"would not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"don\\'t\", \"do not\", content)\n",
    "    content = re.sub(r\"shouldn\\'t\", \"should not\", content)\n",
    "    content = re.sub(r\"needn\\'t\", \"need not\", content)\n",
    "    content = re.sub(r\"hasn\\'t\", \"has not\", content)\n",
    "    content = re.sub(r\"haven\\'t\", \"have not\", content)\n",
    "    content = re.sub(r\"weren\\'t\", \"were not\", content)\n",
    "    content = re.sub(r\"mightn\\'t\", \"might not\", content)\n",
    "    content = re.sub(r\"didn\\'t\", \"did not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    '''content = re.sub(r\"\\'re\", \" are\", content)\n",
    "    content = re.sub(r\"\\'s\", \" is\", content)\n",
    "    content = re.sub(r\"\\'d\", \" would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
    "    content = re.sub(r\"\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
    "    content = re.sub(r\"\\'m\", \" am\", content)'''\n",
    "    return content\n",
    "\n",
    "#Data preprocessing\n",
    "def data_cleaning(content):\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_url(content)\n",
    "    \n",
    "    content = remove_stopwords(content)    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb5cf59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:3\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 1000\n",
    "#Data cleaning\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22792515",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b73b8874",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Mapping rating data to Binary label 1 (+ve) if rating >=7 and 0 (-ve) if rating <=4 and 2 (neutral) if rating = 5 or 6\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRatings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Removing \u001b[39;00m\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m=\u001b[39mdf[df\u001b[38;5;241m.\u001b[39mLabel\u001b[38;5;241m<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Mapping rating data to Binary label 1 (+ve) if rating >=7 and 0 (-ve) if rating <=4 and 2 (neutral) if rating = 5 or 6\n",
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "#Removing \n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Label']]\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad126e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies for feature engineering \n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dff66a",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of word \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    def __call__(self, reviews):\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce7e3f",
   "metadata": {},
   "source": [
    "## Vectoization with Count Vectorizer and TDIDF Vectorizer with Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,1), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,1),min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cc43df",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and Count Vectorizer with unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c2b1f",
   "metadata": {},
   "source": [
    "## Feature Importance with TFIDF vectorizer and Logistic Regression with Unigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310bf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=100:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143d777",
   "metadata": {},
   "source": [
    "## Vectorization with Count Vectorizer and TDIDF Vectorizer with Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c665199",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(2,2), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(2,2),min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742d10f",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and Count Vectorizer with Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38902743",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710b92c2",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and TFIDF Vectorizer with Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a294ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=50:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "df[[\"Reviews\",\"Ratings\",\"Movies\"]][(df['Ratings']>=9)&(df['Reviews_clean'].str.contains(\"bad review\"))].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1badb9f3",
   "metadata": {},
   "source": [
    "## Vectorization with Count Vectorizer and TFIDF Vectorizer with Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(3,3), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(3,3),min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d34da4a",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and Count Vectorizer with Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014b6fd",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and TFIDF Vectorizer with Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faacac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256834d5",
   "metadata": {},
   "source": [
    "## Vectorization with Count Vectorizer and TDIDF Vectorizer with 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(4,4), min_df=10,max_features=500)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(4,4),min_df=10,max_features=500)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a8df4",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and Count Vectorizer with 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6573703",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_count,y_train)\n",
    "lgr.score(x_test_count,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(countvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5397d",
   "metadata": {},
   "source": [
    "## Feature Importance with Logistic Regression and TDIDF Vectorizer with 4-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression()\n",
    "lgr.fit(x_train_tfidf,y_train)\n",
    "lgr.score(x_test_tfidf,y_test)\n",
    "lgr.coef_[0]\n",
    "i=0\n",
    "importantfeature = PrettyTable([\"Feature\", \"Score\"])\n",
    "for feature, importance in zip(tfidfvect.get_feature_names(), lgr.coef_[0]):\n",
    "    if i<=200:\n",
    "        importantfeature.add_row([feature, importance])\n",
    "        i=i+1\n",
    "print(importantfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48152c",
   "metadata": {},
   "source": [
    "## Vectorization with Count Vectorizer and TDIDF Vectorizer with unigram, bigram and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d38c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3), min_df=10,max_features=5000)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=5000)\n",
    "x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f26de",
   "metadata": {},
   "source": [
    "## Feature Selection with Chi squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 5000\n",
    "Number = 1\n",
    "featureselection = PrettyTable([\"Unigram\", \"Bigram\",\"Trigram\"])\n",
    "for category in train['Label'].unique():\n",
    "    features_chi2 = chi2(x_train_tfidf, train['Label'] == category)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidfvect.get_feature_names())[indices]\n",
    "    unigrams = [x for x in feature_names if len(x.split(' ')) == 1]\n",
    "    bigrams = [x for x in feature_names if len(x.split(' ')) == 2]\n",
    "    trigrams = [x for x in feature_names if len(x.split(' ')) == 3]\n",
    "    print(\"%s. %s :\" % (Number,category))\n",
    "    print(\"\\t# Unigrams :\\n\\t. %s\" %('\\n\\t. '.join(unigrams[-N:])))\n",
    "    print(\"\\t# Bigrams :\\n\\t. %s\" %('\\n\\t. '.join(bigrams[-N:])))\n",
    "    print(\"\\t# Trigrams :\\n\\t. %s\" %('\\n\\t. '.join(trigrams[-N:])))\n",
    "    Number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
